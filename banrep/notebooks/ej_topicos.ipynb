{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcionalidad principal de [banrep][pypi_banrep]\n",
    "[pypi_banrep]: https://pypi.org/project/banrep/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos las otras librerías utilizadas en este ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "import plotly.offline as pyo\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y lo que usaremos de la librería [banrep][pypi_banrep]\n",
    "[pypi_banrep]: https://pypi.org/project/banrep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from banrep.corpus import MiCorpus\n",
    "from banrep.io import Textos, leer_palabras\n",
    "from banrep.topicos import Topicos\n",
    "from banrep.utils import crear_directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necesitamos un modelo NLP para español de [spaCy][spacy_models]\n",
    "[spacy_models]: https://spacy.io/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar listas de palabras y expresiones\n",
    "\n",
    "A lo largo de un análisis de textos se suele requerir identificar palabras o expresiones presentes en ellos, bien sea para excluirlas del análisis (ej. *stopwords*), o porque su conteo es útil para generar indicadores (ej. *indicadores de sentimiento*), o porque el análisis se quiere limitar a estudiar aquellos textos que tienen cierto tipo de contenido (ej. *expresiones relacionadas con política económica*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralizar por proyecto\n",
    "\n",
    "En cada proyecto se va a querer tener la flexibilidad para personalizar las palabras y expresiones que se quiere identificar. Para esto se recomienda usar hojas de excel, donde cada hoja puede contener categorías de palabras o expresiones. No es necesario usar un mismo archivo excel, pero facilita tener dichas listas de palabras centralizadas para abrir un solo archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de función: `leer_palabras`\n",
    "\n",
    "Para este proyecto en particular, se quiere identificar (a) palabras *stopwords*, (b) palabras que identifican categorías de *sentimiento*, y (c) frases relacionadas con *incertidumbre en política económica*. Estas están en un archivo excel en las hojas `es_stops`, `es_emocion` y `es_epu` respectivamente.\n",
    "\n",
    "Parámetros `archivo` y `hoja` determinan el archivo excel en disco, y la hoja a usar. (`rutalistas` es, en este caso, una variable que se pasa como argumento al parámetro `archivo`)\n",
    "\n",
    "Parámetro `col_grupo` es el nombre de una columna en la hoja excel que determina el grupo al que pertenecen las palabras.\n",
    "\n",
    "Parámetro `col_palabras` es el nombre de una columna en la hoja excel que contiene las palabras.\n",
    "\n",
    "**Dado que estamos iterando un mismo archivo, este ejemplo asume que las columnas relevantes tienen el mismo nombre en cada hoja.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutalistas = '~/Dropbox/datasets/wordlists/banrep.xlsx'\n",
    "listas = dict()\n",
    "\n",
    "hojas = ['es_stops', 'es_emocion', 'es_epu']\n",
    "for hoja in hojas:\n",
    "    listas[hoja] = leer_palabras(rutalistas, hoja, \n",
    "                                 col_grupo='type', col_palabras='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Generalmente se quiere ignorar palabras comunes a todos los textos, llamadas *stopwords*, por no aportar al entendimiento de los diferentes textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = listas['es_stops'].get(\"stopword\")\n",
    "\n",
    "print(f'{len(stopwords)} palabras stopwords.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de stopwords en excel\n",
    "\n",
    "![](img/stopwords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pertenencia a listas de palabras predefinidas\n",
    "\n",
    "Muchas veces se quiere contabilizar cuantas palabras de cada documento hacen parte de listas de palabras predefinidas. Por ejemplo, puedo tener listas de palabras \"positivas\" y \"negativas\", y querer contar cuantas palabras de los textos que voy a analizar hacen parte de estas listas. Esto sirve, por ejemplo, para crear indicadores de sentimiento basados en el conteo de palabras que pertenecen a emociones \"contrarias\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoja = 'es_emocion'\n",
    "wordlists = listas[hoja]\n",
    "\n",
    "for tipo in wordlists:\n",
    "    print(f'{len(wordlists.get(tipo))} palabras en grupo {tipo} de hoja {hoja}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de emociones en excel\n",
    "\n",
    "![](img/emocion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pertenencia a listas de expresiones predefinidas\n",
    "\n",
    "Al igual que con listas de palabras, a veces se quiere identificar frases de los textos en las que hay presencia de ciertas expresiones. Por ejemplo, un índice reconocido es el [Economic Uncertainty Index][epu], cuyos resultados se basan en el conteo de noticias en las que se encuentren diferentes expresiones relacionadas con incertidumbre en política económica. Por ejemplo, puedo querer identificar expresiones como *Banco de la República*, *déficit fiscal*, *política monetaria*, *inflación de alimentos*, *incertidumbre tributaria*, etc.\n",
    "\n",
    "Identificar estas expresiones sirve también para proyectos en los que se quiere crear un [training set][prodigy] para entrenar un modelo de clasificación basado en *aprendizaje de máquina*, dado que el primer paso para dichos modelos es anotar una serie de frases que sean relevantes para lo que se quiere entrenar.\n",
    "\n",
    "\n",
    "[epu]: http://www.policyuncertainty.com/research.html\n",
    "[prodigy]: https://prodi.gy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoja = 'es_epu'\n",
    "express = listas[hoja]\n",
    "\n",
    "for tipo in express:\n",
    "    print(f'{len(express.get(tipo))} expresiones en grupo {tipo} de hoja {hoja}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo de expresiones en excel\n",
    "\n",
    "![](img/epu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto y filtros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textos a usar\n",
    "\n",
    "Se asume una carpeta en disco en la que hay archivos de texto. Si los textos que quiere utilizar están en archivos binarios como *.pdf*, *.docx*, etc., debe primero [extraer el texto][extraccion].\n",
    "\n",
    "[extraccion]: https://munozbravo.github.io/banrep/uso_extraccion/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de clase: `Textos`\n",
    "\n",
    "Se usa para iterar los archivos de texto en disco.\n",
    "\n",
    "Argumento `rutadatos` especifica ubicación de los textos en disco (parámetro `directorio`).\n",
    "\n",
    "Parámetro `chars` filtra aquellas líneas de texto de cada archivo cuya longitud sea inferior al valor especificado. *69* es valor arbitrario que permite filtrar la mayoría de títulos y subtítulos.\n",
    "\n",
    "Parámetro `parrafos` permite definir si se considera cada párrafo como un documento separado.\n",
    "\n",
    "Parámetro `aleatorio` sirve cuando se quiere iterar los archivos aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutadatos = '../datasets/'\n",
    "\n",
    "datos = Textos(rutadatos, aleatorio=False, chars=69, parrafos=True)\n",
    "print(datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtros: palabras y tokens a ignorar\n",
    "\n",
    "Se obtendrá información detallada de cada palabra gracias a [spaCy][web_spacy], lo que permite filtrar adicionalmente por criterios como [categoría gramatical][universal] (verbos, sustantivos, etc), si es algún tipo de [nombre propio][spacy_ents] (Juan, Colombia, Banco de la República), o si contiene caracteres que no hacen parte del alfabeto (números, monedas, etc). \n",
    "\n",
    "[web_spacy]: https://spacy.io/\n",
    "[universal]: https://universaldependencies.org/es/index.html\n",
    "[spacy_ents]: https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso elimino de cualquier análisis posterior todas las *stopwords* del archivo excel, las categorías gramaticales que identifican números, puntuación y símbolos, y aquellas \"palabras\" o tokens que contengan caracteres que no hacen parte del alfabeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['NUM', 'PUNCT', 'SYM']\n",
    "\n",
    "filtros = dict(stopwords=stopwords, postags=tags, entities=None, is_alpha=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus y sus estadísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el corpus\n",
    "\n",
    "Un *corpus* es un conjunto de documentos, para el cual queremos obtener toda clase de estadísticas.\n",
    "\n",
    "`MiCorpus` es la implementación de un corpus, el cual se inicializa con un modelo [spaCy][spacy_models] y un objeto `Textos`, y opcionalmente con los filtros especificados anteriormente, las listas de palabras que se quiere contar, y expresiones que se quiere encontrar. \n",
    "\n",
    "[spacy_models]: https://spacy.io/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de clase: `MiCorpus`\n",
    "\n",
    "Se usa para inicializar el corpus. Es la estructura más importante de [banrep][pypi_banrep].\n",
    "\n",
    "Argumento `nlp` es el modelo spaCy (parámetro `lang`).\n",
    "\n",
    "Parámetro `datos` es el objeto Textos creado anteriormente en la variable *datos*.\n",
    "\n",
    "Parámetros `filtros`, `wordlists` y `express` ya explicados anteriormente.\n",
    "\n",
    "Parámetro `corta` sirve como filtro adicional, ignorando frases de pocos tokens (palabras, puntuación, símbolos, números, etc. En este ejemplo, frases con menos de 10 tokens).\n",
    "\n",
    "[pypi_banrep]: https://pypi.org/project/banrep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MiCorpus(nlp, datos=datos, filtros=filtros, corta=9, \n",
    "                  wordlists=wordlists, express=express)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas de corpus agregadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = corpus.corpus_stats()\n",
    "stats.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas desagregadas por token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = corpus.corpus_tokens()\n",
    "tokens.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_ngrams = corpus.corpus_ngramed()\n",
    "con_ngrams.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas desagregadas por frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_frases = corpus.frases_stats()\n",
    "stats_frases.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de tópicos y su visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear modelos LDA\n",
    "\n",
    "Los modelos de tópicos se usan para encontrar \"temáticas\" subyacentes en los textos.\n",
    "\n",
    "El parámetro básico a especificar en un modelo es el número de tópicos que se quiere considerar en el resultado.\n",
    "\n",
    "Esta librería usa [Gensim][web_gensim] para la implementación del cálculo de los modelos. En su [documentación][gensim_tuts] encontrará todo lo necesario para correr este tipo de modelos y muchas técnicas adicionales no usadas en esta librería. [banrep][pypi_banrep] simplemente ofrece funciones para correr varios modelos LDA y seleccionar el mejor, todo basado en Gensim.\n",
    "\n",
    "[web_gensim]: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "[gensim_tuts]: https://radimrehurek.com/gensim/tutorial.html\n",
    "[pypi_banrep]: https://pypi.org/project/banrep/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de clase: `Topicos`\n",
    "\n",
    "Se usa para crear modelos lda para diferentes números de tópicos.\n",
    "\n",
    "Argumento `corpus` es una instancia de `MiCorpus`, y `n_topicos` es una lista de números para los cuales se quiere generar un modelo.\n",
    "\n",
    "Parámetro `params` es un diccionario con parámetros que se usan en la [implementación LDA de Gensim][lda_gensim] (gensim.models.ldamodel.LdaModel).\n",
    "\n",
    "Los modelos de tópicos suelen ser evaluados usando una medida llamada *Coherence Score*. Esta medida sugiere qué tan \"interpretables\" son los modelos. Un mayor score es un modelo más \"interpretable\", y por lo tanto mejor.\n",
    "\n",
    "`Topicos` permite acceso a todos los modelos generados, y el mejor según Coherence Score se puede seleccionar usando el método `mejor_modelo`.  \n",
    "\n",
    "[lda_gensim]: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topicos = (5, 10, 15)\n",
    "params = dict(passes=5, alpha='auto', eta='auto', random_state=100)\n",
    "\n",
    "topicos = Topicos(corpus, n_topicos, params)\n",
    "print(topicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = topicos.mejor_modelo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización PyLDAvis (opcional)\n",
    "\n",
    "En análisis de tópicos se suele usar PyLDAvis para visualizar resultados de un modelo.\n",
    "\n",
    "La visualización es muy útil para entender la diferencia entre tópicos (distancia entre los círculos de la gráfica en costado izquierdo), prevalencia de cada tópico (tamaño de cada círculo), y el contenido de cada tópico (palabras más probables desplegadas en costado derecho para cada círculo).\n",
    "\n",
    "El único \"*pero*\" es que suele generar advertencias de uso \"obsoleto\" (DeprecationWarning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Gráfica LDAvis de tópicos y sus palabras\n",
    "bow = list(corpus)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    vis = pyLDAvis.gensim.prepare(modelo, bow, corpus.id2word, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternativa a PyLDAvis (opcional)\n",
    "\n",
    "Para entender diferencia entre tópicos también se puede usar un *heatmap*, en el que cada celda representa la \"distancia\" entre dos tópicos. \n",
    "\n",
    "Este, unido a las *Estadísticas de Modelo de tópicos* mencionadas abajo, provee la misma información que PyLDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "k = modelo.num_topics\n",
    "diferencia, notas = modelo.diff(modelo, distance=\"hellinger\", annotation=False)\n",
    "anno_text = np.around(diferencia, decimals=2)\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=diferencia, annotation_text=anno_text, \n",
    "                                  x=list(range(k)), y=list(range(k)), xgap=1, ygap=1,\n",
    "                                  showscale=True, \n",
    "                                  )\n",
    "\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísticas de Modelo de tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de probabilidad de palabras en cada tópico\n",
    "\n",
    "Un tópico no es más que una serie de palabras con cierta probabilidad de ocurrir. Las palabras con mayor probabilidad son las que permiten \"caracterizar\" un tópico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de método: `palabras_probables`\n",
    "\n",
    "Genera un DataFrame con las palabras más probables de cada tópico en un modelo.\n",
    "\n",
    "Parámetro `modelo` es un modelo LDA y `n` indica cuantas palabras se quiere incuir en el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = topicos.palabras_probables(modelo, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización de palabras con gráficas de barras (opcional)\n",
    "\n",
    "Visualizar las palabras más probables de cada tópico sirve para \"ponerle nombre\" a cada tópico.\n",
    "\n",
    "Aunque esta información es fácilmente visible en `PyLDAvis`, estas son otras opciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "k = modelo.num_topics\n",
    "cols = 2\n",
    "rows = int(np.ceil(k / cols))\n",
    "\n",
    "subi = [(r+1, c+1) for r in range(rows) for c in range(cols)]\n",
    "\n",
    "fig = tools.make_subplots(rows=rows, cols=cols, \n",
    "                          subplot_titles=([f'Tópico {t}' for t in range(k)]),\n",
    "                          print_grid=False,\n",
    "                         )\n",
    "\n",
    "for i, t in enumerate(range(k)):\n",
    "    dfg=palabras.loc[palabras['topico'] == t]\n",
    "    dfg.sort_values(by='probabilidad', inplace=True)\n",
    "    \n",
    "    trace = go.Bar(x=dfg['probabilidad'], y=dfg['palabra'], orientation='h',)\n",
    "    \n",
    "    ix = subi[i]\n",
    "    fig.add_trace(trace, row=ix[0], col=ix[1])\n",
    "\n",
    "fig.layout.update(title='Principales palabras de cada tópico',\n",
    "                  showlegend=False, yaxis=dict(automargin=True, ),\n",
    "                  height=1800, width=1000)\n",
    "\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nubes de palabras (opcional)\n",
    "\n",
    "Las nubes de palabra realmente no ofrecen valor adicional por encima de un gráfico de barras tradicional. Pero si le insisten que por favor genere nubes de palabras bonitas pero inútiles, esta sería una forma de hacerlo.\n",
    "\n",
    "*Para controlar el tipo de letra puede buscar en internet uno que le guste. Este ejemplo usa [CabinSketch descargado de FONT Squirrel][cabin].*\n",
    "\n",
    "[cabin]: https://www.fontsquirrel.com/fonts/cabinsketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc_params =dict(font_path=\"~/Dropbox/datasets/fonts/cabinsketch/CabinSketch-Bold.otf\", \n",
    "                width=800, height=400, prefer_horizontal=0.6, background_color='white')\n",
    "\n",
    "figwc = plt.figure(figsize=(16, 12))\n",
    "figwc.subplots_adjust(hspace=0.05, wspace=0.1)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for t in range(k):\n",
    "        freq = dict(modelo.show_topic(t, topn=20))\n",
    "        wc = WordCloud(**wc_params).generate_from_frequencies(freq)\n",
    "\n",
    "        plt.subplot(rows, cols, t+1).set_title(f\"Tópico {t}\")\n",
    "        plt.plot()\n",
    "        plt.imshow(wc, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    figwc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de probabilidad de tópicos en documentos\n",
    "\n",
    "Usualmente se quiere ver la probabilidad de cada tópico asociada a cada documento. Se puede  pensar un documento como generado de una distribución de probabilidad de tópicos. \n",
    "\n",
    "El método `stats_topicos` devuelve dicha distribución. Para cada documento (fila) muestra la  probabilidad de que hable de cada tópico (columna).\n",
    "\n",
    "Basado en esta distribución, el método también devuelve la \"prevalencia\" de cada tópico: en cuantos documentos del corpus es dominante (el de mayor probabilidad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de método: `stats_topicos`\n",
    "\n",
    "Parámetro `modelo` es un modelo LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopics, dominante = topicos.stats_topicos(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopics.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominante.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de objetos generados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterios\n",
    "\n",
    "El corpus y los modelos de tópicos generan estructuras que pueden ser útiles más adelante, y por lo tanto se quiere guardar a disco.\n",
    "\n",
    "Se quiere usualmente almacenar modelos de \"ngramas\", el \"diccionario\" que contiene todas las palabras únicas existentes en el corpus, y el mejor modelo de tópicos opcionalmente con  su visualización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso de función: `crear_directorio`\n",
    "\n",
    "`crear_directorio` simplemente crea un directorio en disco, en el que se quiere guardar las estructuras generadas. En este caso se quiere crear directorio de salida para estructuras generales, y opcionalmente uno diferente para el mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de salida\n",
    "dirsalida = crear_directorio('topicos')\n",
    "\n",
    "# Crear directorio para mejor modelo.\n",
    "dirmodelo = crear_directorio(dirsalida.joinpath(f'{topicos.top_k:0>2}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar n-gramas y diccionario generados en corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar modelos de bigramas y trigramas\n",
    "corpus.ngrams.get('bigrams').save(str(dirsalida.joinpath('bigrams')))\n",
    "corpus.ngrams.get('trigrams').save(str(dirsalida.joinpath('trigrams')))\n",
    "\n",
    "# Guardar diccionario\n",
    "corpus.id2word.save(str(dirsalida.joinpath('id2word')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar mejor modelo LDA y su visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save(str(dirmodelo.joinpath('topicos.lda')))\n",
    "pyLDAvis.save_html(vis, str(dirmodelo.joinpath('topicos.html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
